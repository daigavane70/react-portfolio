{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityakangune/IT-LP-4-Codes-/blob/main/Chit_6_7_8_CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chit 6 or 7 or 8**\n",
        "\n",
        "### *Name:Aditya Kangune*\n",
        "### *Roll number: 43321*\n",
        "### *Batch: P11*"
      ],
      "metadata": {
        "id": "SOZWUJDdBV9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Problem Statement:*\n",
        "\n",
        "    Implement the Continuous Bag of Words (CBOW) Model for the given (textual document 1) using the below steps:\n",
        "    a. Data preparation\n",
        "    b. Generate training data\n",
        "    c. Train model\n",
        "    d. Output\n",
        "\n"
      ],
      "metadata": {
        "id": "T9f0RSeUJLP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Save the following text in a corona.txt file before"
      ],
      "metadata": {
        "id": "GeoUI8RBlLeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
        "\n",
        "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
        "\n",
        "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t4mNSp1FlTs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "pJO70ku3Ragn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V51q50EbF-T9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('/content/corona.txt','r')\n",
        "corona_data = [text for text in data if text.count(' ') >= 2]\n",
        "vectorize = Tokenizer()"
      ],
      "metadata": {
        "id": "HslLN21ffhSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit data to tokenizer\n"
      ],
      "metadata": {
        "id": "iJiL6QSRfzO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBUwYdBJElVz"
      },
      "outputs": [],
      "source": [
        "vectorize.fit_on_texts(corona_data)\n",
        "corona_data = vectorize.texts_to_sequences(corona_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find total no of words and total no of sentences.\n",
        "total_vocab = sum(len(s) for s in corona_data)\n",
        "word_count = len(vectorize.word_index) + 1\n",
        "window_size = 2"
      ],
      "metadata": {
        "id": "8bCcicjif4i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the pairs of Context words and target words\n",
        "def cbow_model(data, window_size, total_vocab):\n",
        "    total_length = window_size*2\n",
        "    for text in data:\n",
        "        text_len = len(text)\n",
        "        for idx, word in enumerate(text):\n",
        "            context_word = []\n",
        "            target   = []            \n",
        "            begin = idx - window_size\n",
        "            end = idx + window_size + 1\n",
        "            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
        "            target.append(word)\n",
        "            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n",
        "            final_target = np_utils.to_categorical(target, total_vocab)\n",
        "            yield(contextual, final_target)"
      ],
      "metadata": {
        "id": "akOXPLfBgE8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Neural Network model with following parameters :\n",
        "\n",
        "    Model type : sequential\n",
        "    \n",
        "    Layers : Dense , Lambda , embedding. Compile\n",
        "\n",
        "    Options : (loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "EvCwshtogInC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n",
        "model.add(Dense(total_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "for i in range(10):\n",
        "    cost = 0\n",
        "    for x, y in cbow_model(data, window_size, total_vocab):\n",
        "        cost += model.train_on_batch(contextual, final_target)\n",
        "    print(i, cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AJOQ_RBgH0R",
        "outputId": "ed406108-abc5-44fb-c4df-f08eeba195c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "1 0\n",
            "2 0\n",
            "3 0\n",
            "4 0\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 0\n",
            "9 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector file of some word for testing\n",
        "dimensions=100\n",
        "vect_file = open('/content/vectors.txt' ,'w')\n",
        "vect_file.write('{} {}\\n'.format(total_vocab,dimensions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub-YzNuvgHwr",
        "outputId": "992877ba-e94f-4620-c8df-b2bb9f28c308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assign weights to your trained model\n",
        "weights = model.get_weights()[0]\n",
        "for text, i in vectorize.word_index.items():\n",
        "    final_vec = ' '.join(map(str, list(weights[i, :])))\n",
        "    vect_file.write('{} {}\\n'.format(text, final_vec))\n",
        "vect_file.close()"
      ],
      "metadata": {
        "id": "4U-4kh1Ygb_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the vectors created in Gemsim\n",
        "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/vectors.txt', binary = False, limit=100)"
      ],
      "metadata": {
        "id": "Pte5kYhAgb8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the word to get similar type of words\n",
        "cbow_output.most_similar(positive=['virus'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keSXc08Lgb5f",
        "outputId": "fa082128-56c1-4c6c-defb-f1ba73360e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('19', 0.2752428650856018),\n",
              " ('24', 0.2098691165447235),\n",
              " ('of', 0.1796838790178299),\n",
              " ('between', 0.16798871755599976),\n",
              " ('in', 0.14854368567466736),\n",
              " ('period', 0.14530067145824432),\n",
              " ('symptomatic', 0.14341437816619873),\n",
              " ('further', 0.12186173349618912),\n",
              " ('influenza', 0.11704269051551819),\n",
              " ('appearance', 0.1145254373550415)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzwNBXq-gHuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9d0NjxTlgHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjlBj2J1g-oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egcB5gd3g-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQ4T2GJvg-hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0MnRl1jg-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7Io9iYDg-b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxgK4k2Ug-ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIy_gbPPg-XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2XBtQjQg-Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "9bifQMRmmYT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9pUeTjECmYT7"
      },
      "outputs": [],
      "source": [
        "# from keras.preprocessing import text\n",
        "# from keras.utils import np_utils\n",
        "# from keras.preprocessing import sequence\n",
        "# from keras.utils import pad_sequences\n",
        "# import numpy as np\n",
        "# import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CE2OJrPwmYT8"
      },
      "outputs": [],
      "source": [
        "# # Taking random sentences as data\n",
        "\n",
        "# data = \"\"\"\n",
        "# Coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus.\n",
        "\n",
        "# Most people infected with the virus will experience mild to moderate respiratory illness and recover without requiring special treatment. However, some will become seriously ill and require medical attention. Older people and those with underlying medical conditions like cardiovascular disease, diabetes, chronic respiratory disease, or cancer are more likely to develop serious illness. Anyone can get sick with COVID-19 and become seriously ill or die at any age. \n",
        "\n",
        "# The best way to prevent and slow down transmission is to be well informed about the disease and how the virus spreads. Protect yourself and others from infection by staying at least 1 metre apart from others, wearing a properly fitted mask, and washing your hands or using an alcohol-based rub frequently. Get vaccinated when it’s your turn and follow local guidance.\n",
        "# \"\"\"\n",
        "# dl_data = data.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bRoH6Y4_mYT9"
      },
      "outputs": [],
      "source": [
        "# #tokenization\n",
        "# tokenizer = text.Tokenizer()\n",
        "# tokenizer.fit_on_texts(dl_data)\n",
        "# word2id = tokenizer.word_index\n",
        "\n",
        "# word2id['PAD'] = 0\n",
        "# id2word = {v:k for k, v in word2id.items()}\n",
        "# wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "# vocab_size = len(word2id)\n",
        "# embed_size = 100\n",
        "# window_size = 2 \n",
        "\n",
        "# print('Vocabulary Size:', vocab_size)\n",
        "# print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "celNk9LmEvm8"
      },
      "outputs": [],
      "source": [
        "# #tokenization\n",
        "# tokenizer = text.Tokenizer()\n",
        "# tokenizer.fit_on_texts(dl_data)\n",
        "# word2id = tokenizer.word_index\n",
        "\n",
        "# word2id['PAD'] = 0\n",
        "# id2word = {v:k for k, v in word2id.items()}\n",
        "# wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "# vocab_size = len(word2id)\n",
        "# embed_size = 100\n",
        "# window_size = 2 \n",
        "\n",
        "# print('Vocabulary Size:', vocab_size)\n",
        "# print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AAxNYDanInQC"
      },
      "outputs": [],
      "source": [
        "# #generating (context word, target/label word) pairs\n",
        "# def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "#     context_length = window_size*2\n",
        "#     for words in corpus:\n",
        "#         sentence_length = len(words)\n",
        "#         for index, word in enumerate(words):\n",
        "#             context_words = []\n",
        "#             label_word   = []            \n",
        "#             start = index - window_size\n",
        "#             end = index + window_size + 1\n",
        "            \n",
        "#             context_words.append([words[i] \n",
        "#                                  for i in range(start, end) \n",
        "#                                  if 0 <= i < sentence_length \n",
        "#                                  and i != index])\n",
        "#             label_word.append(word)\n",
        "\n",
        "#             x = pad_sequences(context_words, maxlen=context_length)\n",
        "#             y = np_utils.to_categorical(label_word, vocab_size)\n",
        "#             yield (x, y)\n",
        "            \n",
        "# i = 0\n",
        "# for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "#     if 0 not in x[0]:\n",
        "#         # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "#         if i == 10:\n",
        "#             break\n",
        "#         i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rb5dNmoZKZBv"
      },
      "outputs": [],
      "source": [
        "# #model building\n",
        "# import keras.backend as K\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# cbow = Sequential()\n",
        "# cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "# cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "# cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "# cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# print(cbow.summary())\n",
        "\n",
        "# # from IPython.display import SVG\n",
        "# # from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# # SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xs12C3MDK1q4"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(1, 6):\n",
        "#     loss = 0.\n",
        "#     i = 0\n",
        "#     for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "#         i += 1\n",
        "#         loss += cbow.train_on_batch(x, y)\n",
        "#         if i % 100000 == 0:\n",
        "#             print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "#     print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TZ3_QGKVK6Tj"
      },
      "outputs": [],
      "source": [
        "# weights = cbow.get_weights()[0]\n",
        "# weights = weights[1:]\n",
        "# print(weights.shape)\n",
        "\n",
        "# pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UFs2IAn_LAYS"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# distance_matrix = euclidean_distances(weights)\n",
        "# print(distance_matrix.shape)\n",
        "\n",
        "# similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "#                    for search_term in ['disease']}\n",
        "\n",
        "# similar_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GePKWe9AR12s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}